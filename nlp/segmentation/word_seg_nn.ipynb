{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 深度学习的中文分词\n",
    "- 基于word2vec + 神经网络进行中文分词\n",
    "    - 步骤1：使用的sogou的语料库建立初始的字向量。\n",
    "    - 步骤2：读入有标注的训练语料库，处理成keras需要的数据格式。\n",
    "    - 步骤3：根据训练数据建模，使用neural network\n",
    "    - 步骤4：读入无标注的检验语料库，用neural network模型进行分词标注\n",
    "    - 步骤5：检查最终的效果\n",
    "- 参考资料:[中文分词资源](http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90) \n",
    "    [中文分词标注法](http://www.52nlp.cn/the-character-based-tagging-method-of-chinese-word-segmentation) [基于word2vec的中文分词](http://blog.csdn.net/itplus/article/details/17122431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤1：先用sogou语料库生成中文的单字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import itertools\n",
    "from tqdm import trange\n",
    "from pickle import dump, load\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# nltk  \n",
    "import nltk\n",
    "from nltk.probability import FreqDist \n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Reshape, Flatten ,Dropout\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,MaxPooling1D\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8] [1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990]\n"
     ]
    }
   ],
   "source": [
    "text_t = load_sogou_reduced()\n",
    "# number of files in each directory\n",
    "flen = [len(t) for t in text_t.values()]\n",
    "print(list(text_t.keys()), flen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.repeat(list(text_t.keys()),flen)\n",
    "# flatter nested list\n",
    "docs = flatten(text_t.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                txt\n",
       "0      0  　　本报记者陈雪频实习记者唐翔发自上海\\r\\n　　一家刚刚成立两年的网络支付公司，它的目标是...\n",
       "1      0      证券通：百联股份未来5年有能力保持高速增长\\r\\n\\r\\n    深度报告 权威内参...\n",
       "2      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....\n",
       "3      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www....\n",
       "4      0      5月09日消息快评\\r\\n\\r\\n    深度报告 权威内参 来自“证券通”www...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'label': labels, 'txt': docs})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_doc = map(lambda x: ''.join(x.split()), df.txt)\n",
    "sequences = doc_to_seq(corpus_doc, left=1, right=1)\n",
    "tokens_flat = flatten(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>4327816</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>899937</td>\n",
       "      <td>，</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>710057</td>\n",
       "      <td>的</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>417715</td>\n",
       "      <td>。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>227585</td>\n",
       "      <td>一</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq word  idx\n",
       "1325  4327816    \u0000    0\n",
       "4958   899937    ，    1\n",
       "2866   710057    的    2\n",
       "1071   417715    。    3\n",
       "241    227585    一    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character frequency\n",
    "fdist = FreqDist(tokens_flat) \n",
    "w, f = zip(*fdist.items()) # zip(*args) to zip back(unpack)\n",
    "freqdf = pd.DataFrame({'word':w,'freq':f}) \n",
    "freqdf.sort_values(by='freq',ascending =False, inplace=True)\n",
    "freqdf['idx'] = np.arange(len(f))\n",
    "freqdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump(df, open('df.pickle', 'wb'))\n",
    "df = load(open('df.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7116 7116\n"
     ]
    }
   ],
   "source": [
    "word_to_num = dict(zip(freqdf.word, freqdf.idx))\n",
    "num_to_word = dict(zip(freqdf.idx, freqdf.word))\n",
    "print(len(list(fdist.keys())), len(word_to_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word2vec\n",
    "from gensim.models import word2vec\n",
    "def trainW2V(corpus, epochs=50, num_features = 100,\n",
    "             min_word_count=1, num_workers=8,\n",
    "             context=10, sample=1e-5):\n",
    "    global w2v\n",
    "    w2v = word2vec.Word2Vec(workers = num_workers,\n",
    "                          sample = sample,\n",
    "                          size = num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window = context)\n",
    "    np.random.shuffle(corpus)\n",
    "    w2v.build_vocab(corpus)  \n",
    "    for epoch in trange(epochs):\n",
    "        np.random.shuffle(corpus)\n",
    "        w2v.train(corpus)\n",
    "        w2v.alpha *= 0.9  \n",
    "        w2v.min_alpha = w2v.alpha  \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:00<00:00, 36.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# word2vec\n",
    "trainW2V(sequences, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.save('sogou_vectors.bin')\n",
    "w2v.save_word2vec_format('sogou_vectors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A to B is C to what?\n",
      "[('女', 0.4913490116596222),\n",
      " ('性', 0.43872058391571045),\n",
      " ('士', 0.3781505227088928),\n",
      " ('技', 0.35824400186538696),\n",
      " ('魅', 0.35579732060432434),\n",
      " ('仪', 0.349954217672348),\n",
      " ('晓', 0.34426361322402954),\n",
      " ('创', 0.34095433354377747),\n",
      " ('视', 0.3369499742984772),\n",
      " ('伴', 0.33621135354042053)]\n",
      "[('右', 0.6779191493988037),\n",
      " ('螯', 0.37705448269844055),\n",
      " ('侧', 0.3653992712497711),\n",
      " ('翼', 0.3480273187160492),\n",
      " ('初', 0.34055307507514954),\n",
      " ('ㄍ', 0.33485740423202515),\n",
      " ('岁', 0.33165547251701355),\n",
      " ('始', 0.3298236131668091),\n",
      " ('偻', 0.3256300985813141),\n",
      " ('铬', 0.32533490657806396)]\n"
     ]
    }
   ],
   "source": [
    "print('A to B is C to what?')\n",
    "pprint(w2v.most_similar(positive=['新', '男'], negative=['旧']))\n",
    "pprint(w2v.most_similar(positive=['早', '左'], negative=['晚']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word embeddings' look up table matrix，each row is a word's embedding vector,\n",
    "init_wv = []\n",
    "for i in range(freqdf.shape[0]):\n",
    "    init_wv.append(w2v[num_to_word[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义'UNK'为未登陆新字UNKNOWN, 空格为两头padding，并增加两个相应的向量表示\n",
    "char_num = len(init_wv)\n",
    "num_to_word[char_num] = u'<UNK>'\n",
    "word_to_num[u'<UNK>'] = char_num\n",
    "num_to_word[char_num + 1] = u' '\n",
    "word_to_num[u' '] = char_num + 1\n",
    "\n",
    "init_wv.append(np.random.randn(100,))\n",
    "init_wv.append(np.zeros(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump,load\n",
    "dump(init_wv, open('init_wv.pickle', 'wb'))\n",
    "init_wv= load(open('init_wv.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤2：训练数据读取和转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = '../../../data/text'\n",
    "# 读取数据，将格式进行转换为带四种标签 S B M E\n",
    "train_input_file = '%s/icwb2-data/training/msr_training.utf8' % (DATA_ROOT)\n",
    "train_output_file = '%s/icwb2-data/training/msr_training.tagging.utf8' % (DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 tags for character tagging: B(Begin), E(End), M(Middle), S(Single)\n",
    "tag_character_BMES(train_input_file, train_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 分离word 和 label\n",
    "with open(train_output_file) as f:\n",
    "    doc = f.readlines()\n",
    "    train_sentences = [''.join([w[0] for w in line.split()]) for line in doc]\n",
    "    train_tags = [w[-1] for line in doc for w in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 303,  303,  303,   19,   11,   52,  223],\n",
       "       [ 303,  303,   19,   11,   52,  223,   82],\n",
       "       [ 303,   19,   11,   52,  223,   82,   31],\n",
       "       [  19,   11,   52,  223,   82,   31,  275],\n",
       "       [  11,   52,  223,   82,   31,  275,    5],\n",
       "       [  52,  223,   82,   31,  275,    5,    4],\n",
       "       [ 223,   82,   31,  275,    5,    4,   85],\n",
       "       [  82,   31,  275,    5,    4,   85,  192],\n",
       "       [  31,  275,    5,    4,   85,  192,  220],\n",
       "       [ 275,    5,    4,   85,  192,  220,  412],\n",
       "       [   5,    4,   85,  192,  220,  412,    1],\n",
       "       [   4,   85,  192,  220,  412,    1,   79],\n",
       "       [  85,  192,  220,  412,    1,   79,  709],\n",
       "       [ 192,  220,  412,    1,   79,  709,  107],\n",
       "       [ 220,  412,    1,   79,  709,  107,  666],\n",
       "       [ 412,    1,   79,  709,  107,  666,    2],\n",
       "       [   1,   79,  709,  107,  666,    2,  140],\n",
       "       [  79,  709,  107,  666,    2,  140,  432],\n",
       "       [ 709,  107,  666,    2,  140,  432,  224],\n",
       "       [ 107,  666,    2,  140,  432,  224,    5],\n",
       "       [ 666,    2,  140,  432,  224,    5,   10],\n",
       "       [   2,  140,  432,  224,    5,   10,   56],\n",
       "       [ 140,  432,  224,    5,   10,   56,   62],\n",
       "       [ 432,  224,    5,   10,   56,   62,   86],\n",
       "       [ 224,    5,   10,   56,   62,   86,    2],\n",
       "       [   5,   10,   56,   62,   86,    2,  192],\n",
       "       [  10,   56,   62,   86,    2,  192,  220],\n",
       "       [  56,   62,   86,    2,  192,  220,  412],\n",
       "       [  62,   86,    2,  192,  220,  412,    1],\n",
       "       [  86,    2,  192,  220,  412,    1,  326],\n",
       "       [   2,  192,  220,  412,    1,  326,  441],\n",
       "       [ 192,  220,  412,    1,  326,  441,   98],\n",
       "       [ 220,  412,    1,  326,  441,   98,    5],\n",
       "       [ 412,    1,  326,  441,   98,    5,  195],\n",
       "       [   1,  326,  441,   98,    5,  195,  865],\n",
       "       [ 326,  441,   98,    5,  195,  865,  110],\n",
       "       [ 441,   98,    5,  195,  865,  110,   98],\n",
       "       [  98,    5,  195,  865,  110,   98,    2],\n",
       "       [   5,  195,  865,  110,   98,    2, 1135],\n",
       "       [ 195,  865,  110,   98,    2, 1135,   32],\n",
       "       [ 865,  110,   98,    2, 1135,   32,    2],\n",
       "       [ 110,   98,    2, 1135,   32,    2,   22],\n",
       "       [  98,    2, 1135,   32,    2,   22,   35],\n",
       "       [   2, 1135,   32,    2,   22,   35, 1038],\n",
       "       [1135,   32,    2,   22,   35, 1038,    3],\n",
       "       [  32,    2,   22,   35, 1038,    3,  304],\n",
       "       [   2,   22,   35, 1038,    3,  304,  304],\n",
       "       [  22,   35, 1038,    3,  304,  304,  304]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入字符list，输出数字list\n",
    "sent_to_windows(train_sentences[0], word_to_num, window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将所有训练文本转成index window list\n",
    "train_windows = []\n",
    "for line in train_sentences:\n",
    "    train_windows.extend(sent_to_windows(line, word_to_num, window=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4050469 4050469 {'M', 'E', 'B', 'S'}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_windows), len(train_tags), set(train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(train_windows, open('train_windows.pickle', 'wb'))\n",
    "train_windows = load(open('train_windows.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤3：\n",
    "    - Data preprocessing\n",
    "    - 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E': 1, 'B': 0, 'S': 3, 'M': 2}\n",
      "{0: 'B', 1: 'E', 2: 'M', 3: 'S'}\n",
      "number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# 建立两个字典\n",
    "num_to_tag, tag_to_num = list_to_mappings(train_tags)\n",
    "print(tag_to_num)\n",
    "print(num_to_tag)\n",
    "# 将目标变量转为数字index\n",
    "train_labels = [tag_to_num[y] for y in train_tags]\n",
    "\n",
    "nb_classes = len(tag_to_num)\n",
    "print('number of classes:', nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3645422, 7) (3645422, 4) train sequences\n",
      "(405047, 7) (405047, 4) test sequences\n"
     ]
    }
   ],
   "source": [
    "# split and convert dataset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    np.array(train_windows), np.array(train_labels) , train_size=0.9, random_state=1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_y, nb_classes)\n",
    "Y_val = np_utils.to_categorical(val_y, nb_classes)\n",
    "print(train_X.shape, Y_train.shape, 'train sequences')\n",
    "print(val_X.shape, Y_val.shape, 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init variables and hyperparameters\n",
    "init_weight = [np.array(init_wv)]\n",
    "batch_size = 256\n",
    "vocab_size = init_weight[0].shape[0] # 词典大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# one hidden layer neural network，input size = 700，hidden size = 100，output size = 4 classes\n",
    "# 迭代时同时更新神经网络权重，以及词向量\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# 词向量初始化，输入维度：词典大小|V|，输出维度：词向量100\n",
    "# 使用初使词向量可以增加准确率\n",
    "model.add(Embedding(vocab_size, 100, weights=init_weight, input_length=7)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(700, input_dim=100)) # affine layer( fully connected layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes, input_dim=100)) # affine layer( fully connected layer)\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 3280879 samples, validate on 364543 samples\n",
      "Epoch 1/5\n",
      "3280879/3280879 [==============================] - 453s - loss: 0.3722 - acc: 0.8637 - val_loss: 0.2491 - val_acc: 0.9104\n",
      "Epoch 2/5\n",
      "3280879/3280879 [==============================] - 462s - loss: 0.2559 - acc: 0.9087 - val_loss: 0.2033 - val_acc: 0.9278\n",
      "Epoch 3/5\n",
      "3280879/3280879 [==============================] - 481s - loss: 0.2195 - acc: 0.9221 - val_loss: 0.1809 - val_acc: 0.9365\n",
      "Epoch 4/5\n",
      "3280879/3280879 [==============================] - 504s - loss: 0.1975 - acc: 0.9302 - val_loss: 0.1669 - val_acc: 0.9418\n",
      "Epoch 5/5\n",
      "3280879/3280879 [==============================] - 530s - loss: 0.1823 - acc: 0.9356 - val_loss: 0.1588 - val_acc: 0.9453\n"
     ]
    }
   ],
   "source": [
    "# train_X, test_X, Y_train, Y_test\n",
    "print(\"Train...\")\n",
    "earlystop = EarlyStopping(patience=0, verbose=1)\n",
    "result = model.fit(train_X, Y_train, batch_size=batch_size, nb_epoch=1, \n",
    "          validation_split=0.1,callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405047/405047 [==============================] - 12s    \n",
      "Test score: [0.15990888450063415, 0.94513229329309956]\n"
     ]
    }
   ],
   "source": [
    "score = earlystop.model.evaluate(val_X, Y_val, batch_size=batch_size)\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405047/405047 [==============================] - 11s    \n",
      "Test accuracy: 0.945132293289\n"
     ]
    }
   ],
   "source": [
    "# test数据集，准确率0.94\n",
    "classes = earlystop.model.predict_classes(val_X, batch_size=batch_size)\n",
    "acc = np_utils.accuracy(classes, val_y) # \n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump(model, open('model.pickle', 'wb'))\n",
    "model = load(open('model.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤4：用test文本进行预测，评估效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_txt = u'国家食药监总局发布通知称，酮康唑口服制剂因存在严重肝毒性不良反应，即日起停止生产销售使用。\\\n",
    "今天杭州下大雨。随便给我放一首Linkin Park摇滚音乐。'\n",
    "temp_windows = sent_to_windows(temp_txt, word_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s     \n",
      "(76, 7) 76\n"
     ]
    }
   ],
   "source": [
    "# 根据输入得到标注推断\n",
    "temp = predict_tags(temp_windows, temp_txt, model, tag_to_num, num_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '%s/icwb2-data/testing/msr_test.utf8' % (DATA_ROOT)\n",
    "with open(test_file,'r') as f:\n",
    "    test_sentences = f.readlines()\n",
    "    \n",
    "test_X = []\n",
    "for line in test_sentences:\n",
    "    test_windows = sent_to_windows(line, word_to_num)\n",
    "    test_X.extend(test_windows)\n",
    "input_sentences = ''.join(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184358/184358 [==============================] - 16s    \n",
      "(184358, 7) 184358\n"
     ]
    }
   ],
   "source": [
    "test_tags = predict_tags(test_X, input_sentences, model, tag_to_num, num_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input_file = '%s/icwb2-data/testing/msr_test_output.utf8' % (DATA_ROOT)\n",
    "test_output_file = '%s/icwb2-data/testing/msr_test.split.tag2word.utf8' % (DATA_ROOT)\n",
    "with open(test_input_file, 'wb') as f:\n",
    "    f.write(bytes('%s' % (test_tags), 'utf-8'))\n",
    "\n",
    "untag_character_BMES(test_input_file, test_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perl脚本检验的F值为0.929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ../../../data/text/icwb2-data/scripts/score ../../../data/text/icwb2-data/gold/msr_training_words.utf8 ../../../data/text/icwb2-data/gold/msr_test_gold.utf8 ../../../data/text/icwb2-data/testing/msr_test.split.tag2word.utf8 > deep.score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
